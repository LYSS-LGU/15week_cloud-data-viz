#!/usr/bin/env python3
"""
LangChainì„ í™œìš©í•œ Chat Completion API ì‹¤ìŠµ ì˜ˆì œ
==============================================

LangChain í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ OpenAI Chat Completionì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•

ì‚¬ìš©ë²•:
1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •: OPENAI_API_KEY
2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: 
   pip install langchain-openai langchain-community python-dotenv
3. ì‹¤í–‰: python langchain_chat_completion_example.py

ì‘ì„±ì¼: 2025-08-22
"""

import os
import asyncio
import time
from typing import List
from dotenv import load_dotenv

# LangChain imports
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.callbacks import get_openai_callback
from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

class LangChainChatDemo:
    """LangChainì„ í™œìš©í•œ Chat Completion ë°ëª¨"""
    
    def __init__(self):
        """LangChain ChatOpenAI ì´ˆê¸°í™”"""
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.7,
            max_tokens=500
        )
        
    def basic_langchain_chat(self) -> None:
        """ê¸°ë³¸ LangChain Chat ì˜ˆì œ"""
        print("=" * 50)
        print("ğŸ¦œ ê¸°ë³¸ LangChain Chat ì˜ˆì œ")
        print("=" * 50)
        
        # ë©”ì‹œì§€ êµ¬ì„±
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì¹œì ˆí•œ í•œêµ­ì–´ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            HumanMessage(content="LangChainì´ ë¬´ì—‡ì¸ì§€ ê°„ë‹¨íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.")
        ]
        
        # í† í° ì‚¬ìš©ëŸ‰ ì¶”ì 
        with get_openai_callback() as callback:
            response = self.llm.invoke(messages)
            
            print(f"ğŸ“ ì‘ë‹µ: {response.content}")
            print(f"ğŸ’° í† í° ì‚¬ìš©ëŸ‰:")
            print(f"   - í”„ë¡¬í”„íŠ¸ í† í°: {callback.prompt_tokens}")
            print(f"   - ì™„ë£Œ í† í°: {callback.completion_tokens}")
            print(f"   - ì´ í† í°: {callback.total_tokens}")
            print(f"   - ì˜ˆìƒ ë¹„ìš©: ${callback.total_cost:.6f}")

    def prompt_template_example(self) -> None:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ í™œìš© ì˜ˆì œ"""
        print("\n" + "=" * 50)
        print("ğŸ“‹ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì˜ˆì œ")
        print("=" * 50)
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
        template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ {role}ì…ë‹ˆë‹¤. {style} ìŠ¤íƒ€ì¼ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”."),
            ("human", "{question}")
        ])
        
        # ì²´ì¸ êµ¬ì„±
        chain = template | self.llm
        
        # ë‹¤ì–‘í•œ ì—­í• ê³¼ ìŠ¤íƒ€ì¼ë¡œ ì‹¤í–‰
        scenarios = [
            {
                "role": "ìˆ˜í•™ ì„ ìƒë‹˜",
                "style": "ì¹œê·¼í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ìš´",
                "question": "í”¼íƒ€ê³ ë¼ìŠ¤ ì •ë¦¬ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”."
            },
            {
                "role": "ìš”ë¦¬ ì „ë¬¸ê°€",
                "style": "ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ",
                "question": "ê¹€ì¹˜ì°Œê°œ ë§›ìˆê²Œ ë“ì´ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”."
            }
        ]
        
        for i, scenario in enumerate(scenarios, 1):
            print(f"\nğŸ­ ì‹œë‚˜ë¦¬ì˜¤ {i}: {scenario['role']}")
            
            with get_openai_callback() as callback:
                response = chain.invoke(scenario)
                print(f"ì‘ë‹µ: {response.content}")
                print(f"í† í° ì‚¬ìš©: {callback.total_tokens}")

    def conversation_memory_demo(self) -> None:
        """ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ ì˜ˆì œ"""
        print("\n" + "=" * 50)
        print("ğŸ’­ ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬ ì˜ˆì œ")
        print("=" * 50)
        
        # ConversationBufferMemory ì‚¬ìš©
        memory = ConversationBufferMemory(return_messages=True)
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (ë©”ëª¨ë¦¬ í¬í•¨)
        template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ì—¬í–‰ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤. ê³ ê°ì˜ ì—¬í–‰ ê³„íšì„ ë„ì™€ì£¼ì„¸ìš”."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        
        # ì²´ì¸ êµ¬ì„±
        chain = template | self.llm
        
        # ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
        conversations = [
            "ë¶€ì‚° ì—¬í–‰ì„ ê³„íší•˜ê³  ìˆì–´ìš”. 2ë°• 3ì¼ë¡œ ê°€ë ¤ê³  í•©ë‹ˆë‹¤.",
            "í•´ìš´ëŒ€ ë§ê³  ë‹¤ë¥¸ ê´€ê´‘ì§€ë„ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
            "ë§›ì§‘ë„ ì•Œë ¤ì£¼ì‹œë©´ ì¢‹ê² ì–´ìš”.",
            "ìˆ™ë°•ì€ ì–´ë””ê°€ ì¢‹ì„ê¹Œìš”?"
        ]
        
        for i, user_input in enumerate(conversations, 1):
            print(f"\nğŸ‘¤ ì‚¬ìš©ì {i}: {user_input}")
            
            # ë©”ëª¨ë¦¬ì—ì„œ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê°€ì ¸ì˜¤ê¸°
            history = memory.chat_memory.messages
            
            with get_openai_callback() as callback:
                response = chain.invoke({
                    "input": user_input,
                    "history": history
                })
                
                print(f"ğŸ¤– AI: {response.content}")
                print(f"ğŸ’° í† í°: {callback.total_tokens}")
                
                # ë©”ëª¨ë¦¬ì— ëŒ€í™” ì €ì¥
                memory.chat_memory.add_user_message(user_input)
                memory.chat_memory.add_ai_message(response.content)
        
        print(f"\nğŸ“Š ì´ ëŒ€í™” íˆìŠ¤í† ë¦¬ ê¸¸ì´: {len(memory.chat_memory.messages)}")

    def window_memory_demo(self) -> None:
        """ìœˆë„ìš° ë©”ëª¨ë¦¬ ì˜ˆì œ"""
        print("\n" + "=" * 50)
        print("ğŸªŸ ìœˆë„ìš° ë©”ëª¨ë¦¬ ì˜ˆì œ")
        print("=" * 50)
        
        # ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€ (2í„´)
        window_memory = ConversationBufferWindowMemory(
            k=4,  # ìµœê·¼ 4ê°œ ë©”ì‹œì§€ë§Œ ìœ ì§€
            return_messages=True
        )
        
        template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ë„ì›€ì´ ë˜ëŠ” ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."),
            MessagesPlaceholder(variable_name="history"),
            ("human", "{input}")
        ])
        
        chain = template | self.llm
        
        # ê¸´ ëŒ€í™” ì‹œë®¬ë ˆì´ì…˜
        long_conversations = [
            "ì•ˆë…•í•˜ì„¸ìš”! íŒŒì´ì¬ ê³µë¶€ë¥¼ ì‹œì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤.",
            "ë³€ìˆ˜ ì„ ì–¸í•˜ëŠ” ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ë¦¬ìŠ¤íŠ¸ì™€ íŠœí”Œì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ë°˜ë³µë¬¸ì€ ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?",
            "í•¨ìˆ˜ëŠ” ì–´ë–»ê²Œ ì •ì˜í•˜ë‚˜ìš”?",
            "ì²˜ìŒ ì§ˆë¬¸ì´ ë­ì˜€ëŠ”ì§€ ê¸°ì–µí•˜ì‹œë‚˜ìš”?"  # ìœˆë„ìš° ë°–ì˜ ë‚´ìš©
        ]
        
        for i, user_input in enumerate(long_conversations, 1):
            print(f"\nğŸ‘¤ [{i}] {user_input}")
            
            history = window_memory.chat_memory.messages
            print(f"ğŸ“ í˜„ì¬ ë©”ëª¨ë¦¬ í¬ê¸°: {len(history)} ë©”ì‹œì§€")
            
            response = chain.invoke({
                "input": user_input,
                "history": history
            })
            
            print(f"ğŸ¤– {response.content}")
            
            # ìœˆë„ìš° ë©”ëª¨ë¦¬ì— ì €ì¥
            window_memory.chat_memory.add_user_message(user_input)
            window_memory.chat_memory.add_ai_message(response.content)
            
            time.sleep(1)

    async def async_batch_processing(self) -> None:
        """ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ"""
        print("\n" + "=" * 50)
        print("âš¡ ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ")
        print("=" * 50)
        
        # ì—¬ëŸ¬ ì§ˆë¬¸ì„ ë™ì‹œì— ì²˜ë¦¬
        questions = [
            "íŒŒì´ì¬ì˜ ì¥ì  3ê°€ì§€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”.",
            "ë¨¸ì‹ ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?",
            "ì›¹ ê°œë°œì— í•„ìš”í•œ ê¸°ìˆ ìŠ¤íƒì„ ì¶”ì²œí•´ì£¼ì„¸ìš”.",
            "ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ì‹œ ê³ ë ¤ì‚¬í•­ì€?",
            "í´ë¼ìš°ë“œ ì»´í“¨íŒ…ì˜ ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        messages_list = []
        for question in questions:
            messages_list.append([
                SystemMessage(content="ë‹¹ì‹ ì€ IT ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
                HumanMessage(content=question)
            ])
        
        print("ğŸš€ ë™ê¸° ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.time()
        
        # ë™ê¸° ì²˜ë¦¬ (ìˆœì°¨ ì‹¤í–‰)
        sync_responses = []
        for messages in messages_list:
            response = self.llm.invoke(messages)
            sync_responses.append(response.content)
        
        sync_time = time.time() - start_time
        print(f"â±ï¸ ë™ê¸° ì²˜ë¦¬ ì‹œê°„: {sync_time:.2f}ì´ˆ")
        
        print("\nğŸš€ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.time()
        
        # ë¹„ë™ê¸° ì²˜ë¦¬ (ë³‘ë ¬ ì‹¤í–‰)
        async_responses = await self.llm.abatch(messages_list)
        
        async_time = time.time() - start_time
        print(f"â±ï¸ ë¹„ë™ê¸° ì²˜ë¦¬ ì‹œê°„: {async_time:.2f}ì´ˆ")
        print(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒ: {sync_time/async_time:.1f}ë°° ë¹ ë¦„")
        
        # ê²°ê³¼ ë¹„êµ
        print(f"\nğŸ“Š ì²˜ë¦¬ ê²°ê³¼:")
        for i, (question, response) in enumerate(zip(questions, async_responses), 1):
            print(f"{i}. {question}")
            print(f"   ë‹µë³€: {response.content[:100]}...")
            print()

    def streaming_response_demo(self) -> None:
        """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ"""
        print("\n" + "=" * 50)
        print("ğŸŒŠ ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì˜ˆì œ")
        print("=" * 50)
        
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì°½ì˜ì ì¸ ì‘ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content="ë¯¸ë˜ ë„ì‹œì—ì„œ ë²Œì–´ì§€ëŠ” ì§§ì€ ì´ì•¼ê¸°ë¥¼ ë“¤ë ¤ì£¼ì„¸ìš”.")
        ]
        
        print("ğŸ“– ì‹¤ì‹œê°„ ìŠ¤í† ë¦¬ ìƒì„±:")
        print("-" * 30)
        
        # ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì‘ë‹µ ë°›ê¸°
        for chunk in self.llm.stream(messages):
            print(chunk.content, end='', flush=True)
        
        print("\n" + "-" * 30)
        print("âœ… ìŠ¤í† ë¦¬ ìƒì„± ì™„ë£Œ")

    def advanced_prompting_techniques(self) -> None:
        """ê³ ê¸‰ í”„ë¡¬í”„íŒ… ê¸°ë²• ì˜ˆì œ"""
        print("\n" + "=" * 50)
        print("ğŸ§  ê³ ê¸‰ í”„ë¡¬í”„íŒ… ê¸°ë²• ì˜ˆì œ")
        print("=" * 50)
        
        # Chain-of-Thought í”„ë¡¬í”„íŒ…
        cot_template = ChatPromptTemplate.from_messages([
            ("system", "ë‹¹ì‹ ì€ ë…¼ë¦¬ì  ì‚¬ê³ ë¥¼ ì¤‘ì‹œí•˜ëŠ” ë¬¸ì œ í•´ê²° ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            ("human", """
ë‹¤ìŒ ë¬¸ì œë¥¼ ë‹¨ê³„ë³„ë¡œ ì°¨ê·¼ì°¨ê·¼ í’€ì–´ë³´ì„¸ìš”:

{problem}

ê° ë‹¨ê³„ë³„ë¡œ ê³„ì‚° ê³¼ì •ì„ ëª…í™•íˆ ë³´ì—¬ì£¼ì„¸ìš”.
""")
        ])
        
        cot_chain = cot_template | self.llm
        
        problem = """
í•œ íšŒì‚¬ì˜ ì§ì› ìˆ˜ê°€ ì‘ë…„ì— 120ëª…ì´ì—ˆìŠµë‹ˆë‹¤.
ì˜¬í•´ ìƒë°˜ê¸°ì— 20% ì¦ê°€í–ˆê³ , í•˜ë°˜ê¸°ì— 15% ê°ì†Œí–ˆìŠµë‹ˆë‹¤.
í˜„ì¬ ì§ì› ìˆ˜ëŠ” ëª‡ ëª…ì¸ê°€ìš”?
"""
        
        print("ğŸ§® Chain-of-Thought ë¬¸ì œ í•´ê²°:")
        with get_openai_callback() as callback:
            response = cot_chain.invoke({"problem": problem})
            print(response.content)
            print(f"\nğŸ’° í† í° ì‚¬ìš©: {callback.total_tokens}")

    def run_all_demos(self) -> None:
        """ëª¨ë“  ë°ëª¨ ì‹¤í–‰"""
        print("ğŸš€ LangChain Chat Completion ì¢…í•© ì‹¤ìŠµ ì‹œì‘!")
        print("API í‚¤ í™•ì¸:", "âœ… ì„¤ì •ë¨" if os.getenv("OPENAI_API_KEY") else "âŒ ì—†ìŒ")
        
        if not os.getenv("OPENAI_API_KEY"):
            print("\nâŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
            return
        
        demos = [
            ("ê¸°ë³¸ LangChain Chat", self.basic_langchain_chat),
            ("í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿", self.prompt_template_example),
            ("ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬", self.conversation_memory_demo),
            ("ìœˆë„ìš° ë©”ëª¨ë¦¬", self.window_memory_demo),
            ("ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ", self.streaming_response_demo),
            ("ê³ ê¸‰ í”„ë¡¬í”„íŒ… ê¸°ë²•", self.advanced_prompting_techniques),
        ]
        
        for i, (name, demo_func) in enumerate(demos, 1):
            try:
                print(f"\n\nğŸ“Œ [{i}/{len(demos)}] {name} ì‹¤í–‰ ì¤‘...")
                demo_func()
                time.sleep(2)
            except Exception as e:
                print(f"âŒ {name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        # ë¹„ë™ê¸° ë°ëª¨ëŠ” ë³„ë„ ì‹¤í–‰
        print(f"\n\nğŸ“Œ [{len(demos)+1}/{len(demos)+1}] ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ ì¤‘...")
        try:
            asyncio.run(self.async_batch_processing())
        except Exception as e:
            print(f"âŒ ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        print("\n\nğŸ‰ ëª¨ë“  LangChain ë°ëª¨ ì‹¤í–‰ ì™„ë£Œ!")
        print("\nğŸ’¡ LangChain í™œìš© í¬ì¸íŠ¸:")
        print("   1. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ìœ¼ë¡œ ì¬ì‚¬ìš©ì„± í–¥ìƒ")
        print("   2. ë©”ëª¨ë¦¬ ê´€ë¦¬ë¡œ ëŒ€í™” ë§¥ë½ ìœ ì§€")
        print("   3. ì²´ì¸ êµ¬ì„±ìœ¼ë¡œ ë³µì¡í•œ ì›Œí¬í”Œë¡œìš° êµ¬ì¶•")
        print("   4. ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”")
        print("   5. ì½œë°±ìœ¼ë¡œ í† í° ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    demo = LangChainChatDemo()
    
    print("=" * 60)
    print("ğŸ¦œ LangChain Chat Completion API ì‹¤ìŠµ ì˜ˆì œ")
    print("=" * 60)
    print("LangChain í”„ë ˆì„ì›Œí¬ì˜ ê°•ë ¥í•œ ê¸°ëŠ¥ë“¤ì„ ì‹¤ìŠµí•´ë³´ì„¸ìš”!")
    print()
    
    while True:
        print("\nğŸ“‹ ì‹¤í–‰í•  ë°ëª¨ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print("1. ì „ì²´ ë°ëª¨ ì‹¤í–‰")
        print("2. ê¸°ë³¸ LangChain Chat")
        print("3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿")
        print("4. ëŒ€í™” ë©”ëª¨ë¦¬ ê´€ë¦¬")
        print("5. ìœˆë„ìš° ë©”ëª¨ë¦¬")
        print("6. ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬")
        print("7. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ")
        print("8. ê³ ê¸‰ í”„ë¡¬í”„íŒ… ê¸°ë²•")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒ (0-8): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ ì‹¤ìŠµì„ ì¢…ë£Œí•©ë‹ˆë‹¤!")
            break
        elif choice == "1":
            demo.run_all_demos()
        elif choice == "2":
            demo.basic_langchain_chat()
        elif choice == "3":
            demo.prompt_template_example()
        elif choice == "4":
            demo.conversation_memory_demo()
        elif choice == "5":
            demo.window_memory_demo()
        elif choice == "6":
            asyncio.run(demo.async_batch_processing())
        elif choice == "7":
            demo.streaming_response_demo()
        elif choice == "8":
            demo.advanced_prompting_techniques()
        else:
            print("âŒ ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”!")


if __name__ == "__main__":
    main()
